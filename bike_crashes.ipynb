{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Crashes Data Mining / ML Analysis\n",
    "\n",
    "## Leading Question: What variables lead to the most dangerous bike crashes in Chicago?\n",
    "\n",
    "## Process: Build classification model to find most dangerous bike crashes and examine top feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.parse\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "from folium.features import GeoJson\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    learning_curve,\n",
    "    StratifiedKFold,\n",
    "    cross_val_predict,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import set_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "### (Get around limits of API with loop to get data back to 2018)\n",
    "\n",
    "#### limit of 1000 rows per call so call ev month back to 2018 from 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end dates of the desired time range\n",
    "start_date = pd.to_datetime(\"2016-01-01\")  # make this go back to 2016 to get more data\n",
    "end_date = pd.to_datetime(\"2023-05-30\")\n",
    "\n",
    "# Define the interval duration (e.g., one month)\n",
    "interval = pd.DateOffset(months=1)\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate over each interval within the time range\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Construct the URL for the current interval\n",
    "    url = \"https://data.cityofchicago.org/resource/85ca-t3if.csv\"\n",
    "    # Add the date filter to the URL\n",
    "    query = f\"$where=crash_date between '{current_date.date()}' and '{(current_date + interval).date()}'\"\n",
    "    # Add additional filters as needed\n",
    "    query += \" AND first_crash_type='PEDALCYCLIST'\"\n",
    "    # Encode the URL\n",
    "    encoded_url = urllib.parse.quote(f\"{url}?{query}\", safe=\":/?=&\")\n",
    "\n",
    "    # Retrieve the data for the current date interval\n",
    "    df = pd.read_csv(encoded_url)\n",
    "\n",
    "    # Append the data to the list\n",
    "    data.append(df)\n",
    "\n",
    "    # Move to the next interval\n",
    "    current_date += interval\n",
    "\n",
    "# Combine the data from all intervals into a single DataFrame\n",
    "bike_crashes = pd.concat(data, ignore_index=True)\n",
    "\n",
    "bike_crashes.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Bike Crash Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bike crashes by month\n",
    "bike_crashes[\"crash_month\"] = pd.DatetimeIndex(bike_crashes[\"crash_date\"]).month\n",
    "crashes_by_month = bike_crashes[\"crash_month\"].value_counts().sort_index()\n",
    "\n",
    "crashes_by_month.plot(kind=\"bar\", figsize=(10, 5))\n",
    "\n",
    "plt.xlabel(\"Month\")\n",
    "plt.xticks(\n",
    "    range(12),\n",
    "    [\n",
    "        \"Jan\",\n",
    "        \"Feb\",\n",
    "        \"Mar\",\n",
    "        \"Apr\",\n",
    "        \"May\",\n",
    "        \"Jun\",\n",
    "        \"Jul\",\n",
    "        \"Aug\",\n",
    "        \"Sep\",\n",
    "        \"Oct\",\n",
    "        \"Nov\",\n",
    "        \"Dec\",\n",
    "    ],\n",
    "    rotation=45,\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Bike Crashes by Month from 2016 to 2023\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crashes by day of week and hour\n",
    "\n",
    "bike_crashes[\"crash_dayofweek\"] = pd.DatetimeIndex(bike_crashes[\"crash_date\"]).dayofweek\n",
    "bike_crashes[\"crash_hour\"] = pd.DatetimeIndex(bike_crashes[\"crash_date\"]).hour\n",
    "crashes_by_dayhour = bike_crashes.pivot_table(\n",
    "    index=\"crash_hour\", columns=\"crash_dayofweek\", aggfunc=\"size\"\n",
    ")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(crashes_by_dayhour, cmap=\"Blues\")\n",
    "\n",
    "# set x-axis tick labels to day of week\n",
    "plt.gca().set_xticks([i + 0.5 for i in range(7)])\n",
    "plt.gca().set_xticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n",
    "\n",
    "# set y-axis labele to 12 hour time\n",
    "plt.yticks(\n",
    "    range(24),\n",
    "    [\n",
    "        \"12am\",\n",
    "        \"1am\",\n",
    "        \"2am\",\n",
    "        \"3am\",\n",
    "        \"4am\",\n",
    "        \"5am\",\n",
    "        \"6am\",\n",
    "        \"7am\",\n",
    "        \"8am\",\n",
    "        \"9am\",\n",
    "        \"10am\",\n",
    "        \"11am\",\n",
    "        \"12pm\",\n",
    "        \"1pm\",\n",
    "        \"2pm\",\n",
    "        \"3pm\",\n",
    "        \"4pm\",\n",
    "        \"5pm\",\n",
    "        \"6pm\",\n",
    "        \"7pm\",\n",
    "        \"8pm\",\n",
    "        \"9pm\",\n",
    "        \"10pm\",\n",
    "        \"11pm\",\n",
    "    ],\n",
    "    rotation=0,\n",
    ")\n",
    "\n",
    "# add axis labels and title\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"Hour of Day\")\n",
    "plt.title(\"Number of Chicago Bike Crashes by Day and Hour\")\n",
    "\n",
    "# flip y axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# make plot pretty\n",
    "plt.tight_layout()\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bike crash reasons\n",
    "plt.figure(figsize=(10, 5))\n",
    "total_crashes = len(bike_crashes)\n",
    "reason_counts = (\n",
    "    bike_crashes[\"prim_contributory_cause\"].value_counts() / total_crashes * 100\n",
    ")\n",
    "# only plot top 5 reasons\n",
    "reason_counts[:5].plot(kind=\"bar\")\n",
    "# reason_counts.plot(kind=\"bar\")\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel(\"Primary Contributory Cause\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Percentage of Crashes\")\n",
    "plt.title(\"Distribution of Bike Crashes by Weather Condition\")\n",
    "\n",
    "\n",
    "# Add percentage labels to the bars\n",
    "for i, count in enumerate(reason_counts[:5]):\n",
    "    plt.text(i, count, f\"{count:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Make plot pretty\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most dangerous streets\n",
    "plt.figure(figsize=(10, 5))\n",
    "total_crashes = len(bike_crashes)\n",
    "reason_counts = bike_crashes[\"street_name\"].value_counts() / total_crashes * 100\n",
    "# only plot top 8\n",
    "reason_counts[:8].plot(kind=\"bar\")\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel(\"Street Name\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Percentage of Crashes\")\n",
    "plt.title(\"Distribution of Bike Crashes by Street Name\")\n",
    "\n",
    "# Add percentage labels to the bars\n",
    "for i, count in enumerate(reason_counts[:8]):\n",
    "    plt.text(i, count, f\"{count:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Make plot pretty\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Takeaways\n",
    "\n",
    "- Most crashes occur July/Aug which makes sense since more bikers are out. I was surprised the weather didn't dramatically increase winter crashes.\n",
    "- I was surprised that weekly crashes happen most on Tuesday-Thursday. It makes sense that it happens around commuter times but those days specifically I'm guessing have to do with hybrid work people biking to the loop Tue and Thur but would need more data to confirm.\n",
    "- Why are people biking on Western??\n",
    "- 'Failing to Yield to Right of Way' means blowing stop signs so that makes sense as a leading cause of crashing\n",
    "- next step is to explore the kinds of bikers (and drivers) that get in these accidents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the street data as a GeoDataFrame\n",
    "# got from this repo: https://github.com/blackmad/neighborhoods/blob/master/chicago.geojson\n",
    "streets_gdf = gpd.read_file(\n",
    "    \"/Users/benbavonese1/Desktop/coding/bike_crashes/Transportation.geojson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered around Chicago\n",
    "chicago_map = folium.Map(\n",
    "    location=[41.8781, -87.6298], zoom_start=11, tiles=\"cartodbdark_matter\"\n",
    ")\n",
    "# cartodbpositron is light and pretty good, cartodbdark_matter is dark and looks a bit more dramatic\n",
    "\n",
    "# Calculate the number of crashes on each street segment\n",
    "bike_crashes[\"crash_date\"] = pd.to_datetime(bike_crashes[\"crash_date\"])\n",
    "bike_crashes_may = bike_crashes[bike_crashes[\"crash_date\"].dt.month == 5]\n",
    "\n",
    "street_crash_counts = (\n",
    "    bike_crashes.groupby(\"street_name\").size().reset_index(name=\"crash_count\")\n",
    ")\n",
    "\n",
    "# remove street endings from street_crash_counts df to match streets_gdf\n",
    "street_endings = [\n",
    "    \"AVE\",\n",
    "    \"ST\",\n",
    "    \"RD\",\n",
    "    \"DR\",\n",
    "    \"BLVD\",\n",
    "    \"PKWY\",\n",
    "    \"N\",\n",
    "    \"END\",\n",
    "    \"S\",\n",
    "    \"E\",\n",
    "    \"W\",\n",
    "    \"HW\",\n",
    "    \"CT\",\n",
    "]\n",
    "\n",
    "\n",
    "def remove_street_endings(street):\n",
    "    for ending in street_endings:\n",
    "        pattern = rf\"\\b{ending}\\b\"\n",
    "        street = re.sub(pattern, \"\", street)\n",
    "    return street.strip()\n",
    "\n",
    "\n",
    "street_crash_counts[\"street_name\"] = street_crash_counts[\"street_name\"].apply(\n",
    "    remove_street_endings\n",
    ")\n",
    "\n",
    "# rename column\n",
    "street_crash_counts.rename(columns={\"street_name\": \"STREET_NAM\"}, inplace=True)\n",
    "\n",
    "# Merge the street data with the crash counts based on the street segment name\n",
    "streets_gdf_merged = streets_gdf.merge(\n",
    "    street_crash_counts[[\"STREET_NAM\", \"crash_count\"]], on=\"STREET_NAM\", how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "# Set the style function for the choropleth layer\n",
    "def style_function(feature):\n",
    "    count = feature[\"properties\"][\"crash_count\"]\n",
    "    if count is None:\n",
    "        return {\"weight\": 0.0, \"fillOpacity\": 0.0}\n",
    "    else:\n",
    "        # Define the minimum and maximum crash count values to determine the gradient range\n",
    "        min_count = 0  # minimum crash count\n",
    "        max_count = 300  # maximum crash count (adjust as needed) change to 10 and then im calling it a day\n",
    "\n",
    "        # Calculate the color based on the crash count using a linear interpolation\n",
    "        normalized_count = (count - min_count) / (max_count - min_count)\n",
    "        fill_opacity = normalized_count\n",
    "        # print(fill_opacity)\n",
    "        return {\n",
    "            \"color\": \"red\",\n",
    "            \"weight\": normalized_count,\n",
    "            \"fillOpacity\": 0.5,  # make this proportional to the number of crashes\n",
    "        }\n",
    "\n",
    "\n",
    "# Load the latitude and longitude data for lethal bike accidents from another DataFrame\n",
    "lethal_accidents_df = bike_crashes[bike_crashes[\"injuries_fatal\"] == 1][\n",
    "    [\"latitude\", \"longitude\"]\n",
    "]\n",
    "\n",
    "# Iterate over the rows of lethal_accidents_df and add markers for each lethal accident location\n",
    "for _, row in lethal_accidents_df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        icon=folium.Icon(icon=\"times\", prefix=\"fa\", color=\"red\"),\n",
    "    ).add_to(chicago_map)\n",
    "\n",
    "# Create the choropleth layer\n",
    "geojson = GeoJson(streets_gdf_merged, style_function=style_function)\n",
    "chicago_map.add_child(geojson)\n",
    "\n",
    "# Display the map #\n",
    "# chicago_map\n",
    "\n",
    "chicago_map.save(\"chicago_map.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining / Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data for model\n",
    "\n",
    "num_cols = [\n",
    "    \"posted_speed_limit\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "    \"crash_hour\",\n",
    "    \"crash_day_of_week\",\n",
    "    \"crash_month\",\n",
    "    \"lighting_condition\",\n",
    "    \"weather_condition\",\n",
    "    \"road_defect\",\n",
    "    \"traffic_control_device\",\n",
    "    \"trafficway_type\",\n",
    "    \"roadway_surface_cond\",\n",
    "    \"street_name\",\n",
    "    \"street_direction\",\n",
    "]\n",
    "\n",
    "target = [\"target\"]\n",
    "\n",
    "bike_crashes_full = bike_crashes.copy()\n",
    "bike_crashes[\"target\"] = np.where(\n",
    "    (bike_crashes[\"most_severe_injury\"] == \"INCAPACITATING INJURY\")\n",
    "    | (bike_crashes[\"most_severe_injury\"] == \"FATAL INJURY\"),\n",
    "    1,\n",
    "    0,\n",
    ")\n",
    "bike_crashes_full[\"crash_date\"] = bike_crashes_full[\"crash_date\"].astype(str)\n",
    "bike_crashes_full = bike_crashes_full[num_cols + cat_cols + target]\n",
    "\n",
    "\n",
    "# split data in train and validation\n",
    "train, validation = train_test_split(\n",
    "    bike_crashes_full,\n",
    "    test_size=0.5,\n",
    "    stratify=bike_crashes_full[\"target\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train = train.drop(\"target\", axis=1)\n",
    "y_train = train[\"target\"].copy()\n",
    "\n",
    "X_test = validation.drop(\"target\", axis=1)\n",
    "y_test = validation[\"target\"].copy()\n",
    "\n",
    "# data is heavily imbalanced\n",
    "bike_crashes_full[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(transform_output=\"pandas\")  # so we can get the column names\n",
    "\n",
    "# Define the preprocessing steps for numerical and categorical columns\n",
    "num_preprocessor = make_pipeline(KNNImputer(n_neighbors=10), StandardScaler())\n",
    "\n",
    "cat_preprocessor = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False),\n",
    ")\n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "\n",
    "# Create the column transformer to apply different preprocessing steps to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_preprocessor, num_cols),\n",
    "        (\"cat\", make_pipeline(cat_preprocessor, pca), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_combined = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# to get all street names in train and test\n",
    "X_combined_transformed = preprocessor.fit_transform(X_combined)\n",
    "X_train_transformed = X_combined_transformed[: X_train.shape[0]]\n",
    "X_test_transformed = X_combined_transformed[X_train.shape[0] :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    random_state=42,  # Random seed for reproducibility\n",
    "    max_depth=5,  # Maximum depth of each tree\n",
    "    min_samples_leaf=1,  # Minimum number of samples required at each leaf node\n",
    "    min_samples_split=2,  # Minimum number of samples required to split an internal node\n",
    "    max_features=\"sqrt\",  # Number of features to consider when looking for the best split\n",
    "    bootstrap=True,  # Whether bootstrap samples are used when building trees\n",
    "    class_weight=\"balanced\",  # Weight classes inversely proportional to their frequency\n",
    ")\n",
    "\n",
    "forest_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = cross_val_predict(forest_clf, X_test_transformed, y_test, cv=5)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n",
    "\n",
    "# plot confusion matrix of results\n",
    "cm = confusion_matrix(y_test, y_pred, labels=forest_clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=forest_clf.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "# Get predicted probabilities for positive class\n",
    "y_pred_proba = forest_clf.predict_proba(X_train_transformed)[:, 1]\n",
    "\n",
    "# Compute the false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pred_proba)\n",
    "\n",
    "# Compute the AUC (Area Under the Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve (AUC = {:.2f})\".format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do dummy testing as benchmark\n",
    "\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "\n",
    "dummy.fit(X_train_transformed, y_train)\n",
    "\n",
    "dummy_pred = dummy.predict(X_test_transformed)\n",
    "\n",
    "print(\"dummy accuracty: \", accuracy_score(y_test, dummy_pred))\n",
    "\n",
    "y_test.shape\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve graph\n",
    "\n",
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Create the stratified k-fold object\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate the learning curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=forest_clf,\n",
    "    X=X_train_transformed,\n",
    "    y=y_train,\n",
    "    cv=stratified_kfold,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of the training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label=\"Training Accuracy\")\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.3)\n",
    "plt.plot(train_sizes, val_mean, label=\"Validation Accuracy\")\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.3)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model feature importances\n",
    "\n",
    "# Get the feature importances\n",
    "importances = forest_clf.feature_importances_\n",
    "\n",
    "# Get the names of the features\n",
    "feature_names = (\n",
    "    preprocessor.transformers_[1][1][\"pipeline\"]\n",
    "    .steps[-1][1]\n",
    "    .get_feature_names_out(cat_cols)\n",
    ")\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "sorted_indices = importances.argsort()[::-1]\n",
    "sorted_importances = importances[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(sorted_importances[:15])), sorted_importances[:15])\n",
    "plt.xticks(\n",
    "    range(len(sorted_importances[:15])),\n",
    "    sorted_feature_names[:15],\n",
    "    rotation=45,\n",
    "    ha=\"right\",\n",
    ")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\n",
    "    \"Feature Importances (Top 15) for Getting An Incapacitating Injury on a Bike in Chicago\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis + Further Questions\n",
    "\n",
    "### With the best model I found:\n",
    "\n",
    "- Most dangerous crashes depended on:\n",
    "  1. Date being March\n",
    "  2. Certain streets (Couch, Drummond, 18th)\n",
    "  3. Time of day being early in the morning (3/4 am)\n",
    "\n",
    "### Further Questions\n",
    "\n",
    "- Why do bike crashes tend to be more dangerous in March compared to other months?\n",
    "- What are the characteristics of the streets (Couch, Drummond, 18th) that make them more prone to dangerous bike crashes?\n",
    "- Are there any specific factors or conditions during the early morning hours (3/4 am) that contribute to the severity of bike crashes?\n",
    "- Are there any additional variables or features that could potentially be included in the analysis to further improve the model's predictive performance?\n",
    "- Have you considered exploring the relationship between weather conditions and bike crashes to understand how different weather patterns affect the severity of crashes?\n",
    "- Are there any patterns or trends in the data that suggest specific interventions or safety measures that could be implemented to reduce the occurrence of dangerous bike crashes in Chicago?\n",
    "- Could there be any underlying factors or demographic characteristics associated with the identified streets or time periods that contribute to the higher risk of dangerous bike crashes?\n",
    "- Can I find data to explore any correlations between the identified variables (e.g., date, streets, time) and other available information in the dataset, such as traffic volume, road conditions, or visibility, to gain further insights?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
